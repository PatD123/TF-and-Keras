{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(11)Training_Stuffs.ipynb","provenance":[],"authorship_tag":"ABX9TyMm0+7NocvsZpYditX3NgaS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F4TbP2sWMMwd"},"source":["# Import stuff"]},{"cell_type":"code","metadata":{"id":"pt1LQTmyLdVd","executionInfo":{"status":"ok","timestamp":1625145310388,"user_tz":240,"elapsed":2130,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ex7R_yf7MO9X"},"source":["# Key Idea is that weights have to be randomly initialized\n","- Glorot -> None, tanh, softmax, logistic\n","- He -> Relu and Relu variants(RRelu, PRelu, ELU)\n","- Lecun -> Selu\n","  - ***Selu can only be used if the model is sequential, the inputs are standardized, kernel_init is lecun_normal, and all layers have to be Dense***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVhnXBP2Mk9O","executionInfo":{"status":"ok","timestamp":1625095188661,"user_tz":240,"elapsed":188,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"ea048bf1-6277-4b65-b9be-47d87b88bfcf"},"source":["# Use kernel_initializer\n","tf.keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_uniform\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.core.Dense at 0x7fec074a1050>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"-7Km5F8EOZoI"},"source":["**SELU > ELU > LeakyRelu(other variants too) > Relu > tanh > logistic**"]},{"cell_type":"markdown","metadata":{"id":"suAFVk-WPDSJ"},"source":["# To apply activation functions onto models\n","- Place the activation function right after the layer you want to apply it to for variants of LeakyRelu and LeakyRelu"]},{"cell_type":"code","metadata":{"id":"c_PV4IXLPBNi"},"source":["# LeakyRelus, PRelus\n","tf.keras.layers.Dense(30)\n","tf.keras.layers.LeakyReLU(alpha=0.2) # 0.2 is often pretty good\n","\n","# Selu -> Self-normalizes the whole model so it solves exploding/vanishing gradients\n","tf.keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_uniform\") # Have to have lecun"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-us_yVFTqXS"},"source":["# Using Batch Normalization\n","- Always use this because it has a lot of good stuff and has a LOT OF BENEFITS\n","- Normalizes the inputs of the layers so if you do that right after the input/Flatten, you don't need to use StandardScaler to normalize\n","  - Has four parameters gamma(output scale), beta(output offset), mu(mean), sigma(std dev)"]},{"cell_type":"code","metadata":{"id":"_RuYJegcTwdr"},"source":["model = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28,28)),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_uniform\"),   \n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_uniform\"),\n","  tf.keras.layers.BatchNormalization(),\n","  tf.keras.layers.Dense(10, activation=\"softmax\")                           \n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ColnUSCuUpla","executionInfo":{"status":"ok","timestamp":1625096891426,"user_tz":240,"elapsed":179,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"d68559c0-dc3e-4aab-f732-9c8fbfdb816e"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_1 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 784)               3136      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 300)               235500    \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 300)               1200      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 100)               30100     \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 100)               400       \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 10)                1010      \n","=================================================================\n","Total params: 271,346\n","Trainable params: 268,978\n","Non-trainable params: 2,368\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B6uYeT35XlV0"},"source":["# Reusing pretrained stuffs\n","- Already trained a model to classify different fruits and veggies in a picture. Now you want to classify cars. The two models seem pretty similar so cna just reuse part of the first model."]},{"cell_type":"markdown","metadata":{"id":"_yv-3i1cauK_"},"source":["When you want to reuse layers, note that the bottom hidden layers are the most important and as you go further to the output layer, they become less and less important. Also, when you reuse layers, freeze the bottom few layers to prevent those hyperparameters from being tweaked.\n","\n","\n","Certain situations, though similar to the original model classification, might need you to drop certain layers and freeze others."]},{"cell_type":"code","metadata":{"id":"X0uxXbV4sK6O"},"source":["# Let's pretend there we already had a good classifier from the MNIST fashion stored in my_model_A.h5\n","model_A = tf.keras.models.load_model(\"my_model_A.h5\") # Loads model\n","model_A_clone = tf.keras.models.clone_model(model_A)  # Clone the model because if you don't, when you manipulate the weights of model_B_on_A,\n","                                                      # you're going to be changing model_A's weights too.\n","model_A_clone.set_weights(model_A.get_weights())      # When you clone, it only returns the structure, not the weights. So, here you get and set the weights\n","\n","model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1]) # Simply pass in the layers. This basically takes in all layers except for the last one(output)\n","                                                              # so you are basically reusing the layers.\n","model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\")) # Add in the output layer.\n","\n","for layer in model_B_on_A.layers[:-1]:  # Freezes the reused layers\n","  layer.trainable = False;\n","\n","model_B_on_A.compile(loss=\"binary_crossentropy\", # Binary_ce because it was the situation at hand\n","                     otpimizer=\"sgd\",\n","                     metrics=[\"accuracy\"])\n","\n","# Then you just fit the ting"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CyysL0xczCNE"},"source":["## ***Big thing to notice here is that this binary classification is really simple and book says that transfer learning doesn't work well for simple models. USE IT FOR DEEP DEEP NEURAL NETS***"]},{"cell_type":"markdown","metadata":{"id":"1een6n9W7Jsu"},"source":["# Situations where you don't have much labeled training data\n","\n","What you do is you perform an auxiliary task. This means to train your model on a similar task that you have lots of labeled training data for. In this way, this data will provide good lower hidden layers for your model's original task. Then you can just reuse the lower layers from the aux task for your original task"]},{"cell_type":"markdown","metadata":{"id":"zr4P8eQK7nmL"},"source":["# Fast optimizers\n","\n","We've alrady explored a few ways to make training faster and better: kernel_init, better activation functions, Batch Normalization, and reusing parts of similar models.\n","\n","Next we will look at better optimizers than simple SGD/GD\n","- Best is Adam"]},{"cell_type":"markdown","metadata":{"id":"YY3oLjEL8pRn"},"source":["## Momentum\n","- As you go down curve, you can go faster and faster(you input the momentum cap(0.9))\n","- Can get to the valley a lot quicker\n","- Can roll past local_minimas"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eeUe5EmX9D-n","executionInfo":{"status":"ok","timestamp":1625107504560,"user_tz":240,"elapsed":218,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"906ace9d-cce0-40f1-f07a-00ddc52c2240"},"source":["optimizer = tf.keras.optimizers.SGD(lr=0.003, momentum=0.9)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5U9gWi9-AZAq"},"source":["## Adam\n","- Generally, always use Adam"]},{"cell_type":"code","metadata":{"id":"QQe8lHLeAemZ"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003, beta_1=0.9, beta_2=0.999)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AhDh1L8WNKG3"},"source":["# Regularizers\n","- We have already seen Batch Normalization which does also help out with regularization"]},{"cell_type":"code","metadata":{"id":"yqw-kJuJNQJj","executionInfo":{"status":"ok","timestamp":1625145365844,"user_tz":240,"elapsed":162,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["layer = tf.keras.layers.Dense(300, activation=\"elu\",\n","                              kernel_initializer=\"he_uniform\",\n","                              kernel_regularizer=tf.keras.regularizers.l2(0.01))"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JfVmf-QsNuil"},"source":["## You are going to be initializing a lot of activations and layers and because most layers are going to be pretty much the save, you can use functools"]},{"cell_type":"code","metadata":{"id":"KFQoD08aN2m1","executionInfo":{"status":"ok","timestamp":1625145868784,"user_tz":240,"elapsed":187,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["from functools import partial\n","\n","RegularizedDense = partial(tf.keras.layers.Dense,\n","                           activation=\"elu\",\n","                           kernel_initializer=\"he_uniform\",\n","                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n","\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28,28)),\n","  RegularizedDense(300),\n","  RegularizedDense(100),                            \n","  RegularizedDense(300, activation=\"softmax\",\n","                   kernel_initializer=\"glorot_uniform\")\n","])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ko9ExPntQSoG"},"source":["## Dropout regularizers\n","- Dropout works because it makes each neuron stand on its own, making it become more independent, which then makes the whole entire model more robust.\n","- ***If you are going to be using SELU, use alpha dropout.***\n","- **If regular Dropout is too strong, only use one dropout and place that right after the last hidden layer. You are only supposed to place it in the top 1-3 layers.**"]},{"cell_type":"code","metadata":{"id":"kwNnp4dWRdao","executionInfo":{"status":"ok","timestamp":1625146785626,"user_tz":240,"elapsed":157,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["RegularizedDense = partial(tf.keras.layers.Dense,\n","                           activation=\"selu\",\n","                           kernel_initializer=\"lecun_uniform\",\n","                           kernel_regularizer=tf.keras.regularizers.l2(0.001),\n","                           )\n","\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28,28)),\n","  tf.keras.layers.BatchNormalization(),\n","  RegularizedDense(300),\n","  tf.keras.layers.BatchNormalization(),\n","  RegularizedDense(100),\n","  tf.keras.layers.AlphaDropout(0.3),\n","  tf.keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n","])"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8F4qYdgEXALu"},"source":["## Monte Carlo\n","- You should always use it for dropout but i don't know how to implement"]},{"cell_type":"code","metadata":{"id":"hKKte8EkXG9t"},"source":["#def predict_proba(X, model, num_samples):\n","#    preds = [model(X, training=True) for _ in range(num_samples)]\n","#    return np.stack(preds).mean(axis=0)\n","#     \n","#def predict_class(X, model, num_samples):\n","#    proba_preds = predict_proba(X, model, num_samples)\n","#    return np.argmax(proba_preds, axis=1)\n","#\n","#y_pred = predict_class(X_test, model, 100) ----> predicting with MC\n","#acc = np.mean(y_pred == y_test) ----> Answer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YkpcnDsvXqSV"},"source":["# Notes"]},{"cell_type":"markdown","metadata":{"id":"CxPrGtn_XraI"},"source":["- If you need a sparse model, use l1 reg\n"]}]}
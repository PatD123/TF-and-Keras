{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(13)Play_With_Data.ipynb","provenance":[],"authorship_tag":"ABX9TyPd1jmYwl0m29yRLrlHwUIs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tdBxw5Nn5ySP"},"source":["# Import stuffs"]},{"cell_type":"code","metadata":{"id":"nHpz38mv5c5y","executionInfo":{"status":"ok","timestamp":1625406276585,"user_tz":240,"elapsed":170,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","def print_(dataset):\n","  for item in dataset:\n","    print(item)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSiC0GHQ_D3g"},"source":["# ***Dataset methods do not do anything to the original dataset***"]},{"cell_type":"markdown","metadata":{"id":"TacY2jgk534c"},"source":["# Examples"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tnwVtNl56uZ","executionInfo":{"status":"ok","timestamp":1625404251771,"user_tz":240,"elapsed":150,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"0dfcb917-0293-4466-9d8a-895b035bf7c4"},"source":["x = tf.range(10)\n","dataset = tf.data.Dataset.from_tensor_slices(x) # basically slices x up so that each of x's elements is individually in dataset.\n","print_(dataset)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tf.Tensor(0, shape=(), dtype=int32)\n","tf.Tensor(1, shape=(), dtype=int32)\n","tf.Tensor(2, shape=(), dtype=int32)\n","tf.Tensor(3, shape=(), dtype=int32)\n","tf.Tensor(4, shape=(), dtype=int32)\n","tf.Tensor(5, shape=(), dtype=int32)\n","tf.Tensor(6, shape=(), dtype=int32)\n","tf.Tensor(7, shape=(), dtype=int32)\n","tf.Tensor(8, shape=(), dtype=int32)\n","tf.Tensor(9, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7z6Wdxxv6bb-","executionInfo":{"status":"ok","timestamp":1625404272682,"user_tz":240,"elapsed":162,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"0bb69267-906b-44ae-e04c-1dbd8c44463f"},"source":["dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n","# dataset.repeat(3) => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, . . .9]\n","# dataset.repeat(3).batch(7) => ||\n","#                                \\/\n","print_(dataset)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n","tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n","tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n","tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EVmLOU1U5xrF"},"source":["# Use lambda as a function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enChsLl5-k6c","executionInfo":{"status":"ok","timestamp":1625404278968,"user_tz":240,"elapsed":167,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"66694981-bf45-4b53-da8b-f6feebd9cec5"},"source":["x = tf.range(6)\n","dataset = tf.data.Dataset.from_tensor_slices(x)\n","dataset = dataset.map(lambda x: x*2)\n","print_(dataset)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tf.Tensor(0, shape=(), dtype=int32)\n","tf.Tensor(2, shape=(), dtype=int32)\n","tf.Tensor(4, shape=(), dtype=int32)\n","tf.Tensor(6, shape=(), dtype=int32)\n","tf.Tensor(8, shape=(), dtype=int32)\n","tf.Tensor(10, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ceu3tv2s_qli"},"source":["## Filter function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vnLKMtM__tM7","executionInfo":{"status":"ok","timestamp":1625404287542,"user_tz":240,"elapsed":180,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"c141dcc7-d707-4d4b-f307-6f01a9596768"},"source":["dataset = dataset.filter(lambda x: x < 10) # Filters everything that is not x < 10\n","print_(dataset)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["tf.Tensor(0, shape=(), dtype=int32)\n","tf.Tensor(2, shape=(), dtype=int32)\n","tf.Tensor(4, shape=(), dtype=int32)\n","tf.Tensor(6, shape=(), dtype=int32)\n","tf.Tensor(8, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hd9W6n9VorQG"},"source":["## Shuffling data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__55BXNzotED","executionInfo":{"status":"ok","timestamp":1625404389358,"user_tz":240,"elapsed":164,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"78be9232-dbc4-4bbe-b031-057038bf18ff"},"source":["dataset = tf.data.Dataset.range(10).repeat(3)\n","dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n","print_(dataset)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n","tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n","tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n","tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n","tf.Tensor([3 6], shape=(2,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o8Tft_bCucEH"},"source":["# California stuffs"]},{"cell_type":"markdown","metadata":{"id":"vP-Y9Da6wV-J"},"source":["## Preprocessing pg 292-294"]},{"cell_type":"markdown","metadata":{"id":"dgxEjtfTwZpc"},"source":["### Import stuffs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0_hifjewYIF","executionInfo":{"status":"ok","timestamp":1625406169777,"user_tz":240,"elapsed":3177,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"4916d47c-2ba3-4793-8187-ac779079b4e9"},"source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","housing = fetch_california_housing()\n","X_train_full, X_test, y_train_full, y_test = train_test_split(\n","    housing.data, housing.target.reshape(-1, 1), random_state=42)\n","X_train, X_valid, y_train, y_valid = train_test_split(\n","    X_train_full, y_train_full, random_state=42)\n","\n","scaler = StandardScaler()\n","scaler.fit(X_train)\n","X_mean = scaler.mean_\n","X_std = scaler.scale_"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"mOJgjWAmwhlY"},"source":["### Split up the training into smaller csv files for tensorflow to read because if the files are initially huge, it's a good thing to do"]},{"cell_type":"code","metadata":{"id":"lIxWqINAwq3O","executionInfo":{"status":"ok","timestamp":1625406248065,"user_tz":240,"elapsed":166,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n","    housing_dir = os.path.join(\"datasets\", \"housing\")\n","    os.makedirs(housing_dir, exist_ok=True)\n","    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n","\n","    filepaths = []\n","    m = len(data)\n","    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n","        part_csv = path_format.format(name_prefix, file_idx)\n","        filepaths.append(part_csv)\n","        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n","            if header is not None:\n","                f.write(header)\n","                f.write(\"\\n\")\n","            for row_idx in row_indices:\n","                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n","                f.write(\"\\n\")\n","    return filepaths"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_lzZQTywsp2","executionInfo":{"status":"ok","timestamp":1625406280071,"user_tz":240,"elapsed":339,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["train_data = np.c_[X_train, y_train]\n","valid_data = np.c_[X_valid, y_valid]\n","test_data = np.c_[X_test, y_test]\n","header_cols = housing.feature_names + [\"MedianHouseValue\"]\n","header = \",\".join(header_cols)\n","\n","train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n","valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n","test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kqJdWizw6Ii"},"source":["### Preprocessing"]},{"cell_type":"code","metadata":{"id":"eJqP6POCw78y","executionInfo":{"status":"ok","timestamp":1625406328426,"user_tz":240,"elapsed":161,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["n_inputs = 8 # X_train.shape[-1]\n","\n","@tf.function\n","def preprocess(line):\n","    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n","    fields = tf.io.decode_csv(line, record_defaults=defs)\n","    x = tf.stack(fields[:-1])\n","    y = tf.stack(fields[-1:])\n","    return (x - X_mean) / X_std, y"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"jPwqYQB-xEUT","executionInfo":{"status":"ok","timestamp":1625406329665,"user_tz":240,"elapsed":226,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n","                       n_read_threads=None, shuffle_buffer_size=10000,\n","                       n_parse_threads=5, batch_size=32):\n","    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n","    dataset = dataset.interleave(\n","        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n","        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n","    dataset = dataset.shuffle(shuffle_buffer_size)\n","    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n","    dataset = dataset.batch(batch_size)\n","    return dataset.prefetch(1)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hjEwxzDF2LOt"},"source":["# TFRecord and Proto(pg 297-300)"]},{"cell_type":"markdown","metadata":{"id":"Da8tEkXG2TXs"},"source":["# For standardization you can either use:\n","1. StandardScaler\n","2. tf.keras.layers.Normalization\n","  - Have to first create a normalization layer, pass in stuff to its .adapt() function, and then you can add it to your model\n","3. Create your own(pg 301)"]},{"cell_type":"markdown","metadata":{"id":"sT_4g2iZ2xVD"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"KyHq0bVj3MF0"},"source":["# One-hot encoding with Cali ocean_proximity"]},{"cell_type":"code","metadata":{"id":"xmBQs9lE3Q-_","executionInfo":{"status":"ok","timestamp":1625408187185,"user_tz":240,"elapsed":152,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["# Because there weren't that many categroies in ocean_proximity, we can just make a map-dictionary\n","vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n","indices = tf.range(len(vocab), dtype=tf.int64)\n","table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices) # If in a text file, use TextFileInitializer\n","num_oov_buckets = 2\n","table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets=num_oov_buckets)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YR5G2bzD5u_n","executionInfo":{"status":"ok","timestamp":1625408729891,"user_tz":240,"elapsed":174,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"c4c642ff-fe59-4002-fc89-82c43e7ad2d1"},"source":["categories = tf.constant([\"NEAR BAY\",\"DESERT\", \"INLAND\", \"INLAND\"])\n","indices = table.lookup(categories)\n","print(indices)\n","cat_1hot = tf.one_hot(indices, len(vocab) + num_oov_buckets)\n","print(cat_1hot)\n","\n","# What the oov does is that it will create an extra category for every word that was not found in the vocab list\n","# However, if the the amount of words not found within the vocab list exceeds the oov, then collisions.\n","\n","# If the word was not found, then its category starts at the len(vocab) => 5 in this case"],"execution_count":38,"outputs":[{"output_type":"stream","text":["tf.Tensor([3 5 1 1], shape=(4,), dtype=int64)\n","tf.Tensor(\n","[[0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0.]\n"," [0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0.]], shape=(4, 7), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RYq6ToWw6xe7"},"source":["# Embeddings\n","- Use when there are a bunch of categories that you need to one_hot encode but it would be too much to do with one_hot encoding.\n","  - If the number of categories is less than 10, do one_hot.\n","  - If 10<x<50, if you think you can do it manually, then try both\n","  -x>50, use embeddings"]},{"cell_type":"markdown","metadata":{"id":"ChFahcxV7_gY"},"source":["## Notes on Embeddings and what they do\n","- Form of representation learning.\n","- ***Categories in embeddings are trainable dense vectors which means they can learn similarities between one another.***\n","- Initially, categories are spread out randomly in the embedding space.\n","  - As the model learns, it sees that some categories are similar to each other while others are just completely different.\n","    - \"Rain\" and \"Water\" would be pushed closer together while \"Fire\" would be pushed farther away."]},{"cell_type":"markdown","metadata":{"id":"eBfNGHWi9-jL"},"source":["## Manual Embeddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rpg0IuGS99WG","executionInfo":{"status":"ok","timestamp":1625410022029,"user_tz":240,"elapsed":146,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"cc5034c2-36b7-42eb-8b97-cdef71515300"},"source":["embedding_dim = 2\n","embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n","embedding_matrix = tf.Variable(embed_init)\n","embedding_matrix, len(vocab) + num_oov_buckets # Creates an embedding matrix--where each categorical word is at in the 2D embedding space"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n"," array([[0.3509872 , 0.679674  ],\n","        [0.24280798, 0.52020407],\n","        [0.5276462 , 0.5308013 ],\n","        [0.7144053 , 0.7121148 ],\n","        [0.8601637 , 0.54973423],\n","        [0.3824227 , 0.19751704],\n","        [0.06983769, 0.268381  ]], dtype=float32)>, 7)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uBNTDq1b_AFb","executionInfo":{"status":"ok","timestamp":1625410029213,"user_tz":240,"elapsed":195,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"ebe4d096-5c8e-4a7b-b8fb-8934fac1a7cc"},"source":["tf.nn.embedding_lookup(embedding_matrix, indices)\n","# Basically just looks up the positions at the given indices.\n","# First was 3 so what is at index=3 in the embedding_matrix"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n","array([[0.7144053 , 0.7121148 ],\n","       [0.3824227 , 0.19751704],\n","       [0.24280798, 0.52020407],\n","       [0.24280798, 0.52020407]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"A6hV3y5N_nes"},"source":["## Keras Embeddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jEkDc3WZAh08","executionInfo":{"status":"ok","timestamp":1625410381074,"user_tz":240,"elapsed":171,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}},"outputId":"dffbe4dd-2072-478b-b1dc-c808b19abf7f"},"source":["embedding = tf.keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets,\n","                          output_dim=2) # Basically just does whatever we did above.\n","embedding(indices)"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n","array([[-0.02710378,  0.04603578],\n","       [ 0.02083026,  0.04809587],\n","       [ 0.01589933, -0.04899972],\n","       [ 0.01589933, -0.04899972]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"6Oz-B0sfAyE_"},"source":["## Full model with embeddings"]},{"cell_type":"code","metadata":{"id":"sbHq-jVyAz71","executionInfo":{"status":"ok","timestamp":1625410774017,"user_tz":240,"elapsed":218,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["numerical_inputs = tf.keras.layers.Input(shape=[8])\n","categories = tf.keras.layers.Input(shape=[], dtype=tf.string)\n","indices = tf.keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n","cat_embed = embedding(indices)\n","encoded_inputs = tf.keras.layers.concatenate([numerical_inputs, cat_embed])\n","outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n","model = tf.keras.Model(inputs=[numerical_inputs, categories],\n","                       outputs=[outputs])\n","\n","# If you don't want to manually take in the inputs, use TextVectorization\n","# Call its method\tto make\tit extract the vocabulary\tfrom\ta\tdata\tsample\t(it\twill\ttake\tcare\tof\tcreating\tthe\tlookup\ttable\tfor\tyou).\t\n","# Then\tyou\tcan\tadd it\tto\tyour\tmodel,\tand\tit\twill\tperform\tthe\tindex\tlookup\t(replacing\tthe\t\tlayer\tin\tthe\tprevious\tcode example)."],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wSUS2XarQpp7"},"source":["# More preprocessing using Keras"]},{"cell_type":"code","metadata":{"id":"mw90lk0GQr8e","executionInfo":{"status":"ok","timestamp":1625415005509,"user_tz":240,"elapsed":220,"user":{"displayName":"ThatOneGuy 4","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifdiA3IY1W_lxhJLvpVcINYel5dPvvCji8tMXk=s64","userId":"01730018351539431740"}}},"source":["from keras.layers.experimental.preprocessing import Normalization, Discretization, PreprocessingLayer\n","normalization = Normalization()\n","discretization = Discretization()\n","pipeline = PreprocessingLayer([normalization, discretization])\n","pipeline.adapt(data_sample)\n","\n","# Word stuff with text vectorization on pg 306\n","\n"],"execution_count":63,"outputs":[]}]}
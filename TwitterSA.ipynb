{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TwitterSA.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vipXgLmqzHwr"},"source":["# Keys"]},{"cell_type":"code","metadata":{"id":"MdW0W93YnF-W","executionInfo":{"status":"ok","timestamp":1628262400269,"user_tz":240,"elapsed":281,"user":{"displayName":"labesh baral","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgKfkRAcC4q8O4AOIGKbBTAqtxCx15FOR_A-b0=s64","userId":"09665838403334475686"}}},"source":["# API Key = bi3ryowpzlt4hLnvwvAocXr3o\n","\n","# API Secret Key = QM46QJCLFvRw6xF4ui6LKg7CmiqmzopXVgThn3uOZTFplZPoaL\n","\n","# Bearer Token = AAAAAAAAAAAAAAAAAAAAAPXKSAEAAAAAtH02CwbHRGKHZj3Oy4tt54teXp8%3DBXqGkZ5e4lmoCMMOlIqtC1UMFSpO0KdKjPI0hJIYab5Yrsu1UB\n","\n","# YouTube Video: https://www.youtube.com/watch?v=wlnx-7cm4Gg\n","# Variables that contains the user credentials to access Twitter API \n","ACCESS_TOKEN = \"1421102110289141763-2XmBiSQcmCUVPOflo1JYp1wOXgogSt\"\n","ACCESS_TOKEN_SECRET = \"q4QHQJGrQ9glpx4MM18KFaBfk7jSq5azcckrPpslgaLUQ\"\n","CONSUMER_KEY = \"bi3ryowpzlt4hLnvwvAocXr3o\"\n","CONSUMER_SECRET = \"QM46QJCLFvRw6xF4ui6LKg7CmiqmzopXVgThn3uOZTFplZPoaL\""],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ah_KTX-dzJ89"},"source":["# Import Stuffs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6jcjT4M0_fT","executionInfo":{"status":"ok","timestamp":1628262381444,"user_tz":240,"elapsed":3922,"user":{"displayName":"labesh baral","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgKfkRAcC4q8O4AOIGKbBTAqtxCx15FOR_A-b0=s64","userId":"09665838403334475686"}},"outputId":"87832447-dc78-49d8-9d08-9cc1e947844f"},"source":["!pip install pytorch_pretrained_bert\n","!pip install bertModel "],"execution_count":10,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.9.0+cu102)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.18.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n","Requirement already satisfied: botocore<1.22.0,>=1.21.15 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.21.15)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.15->boto3->pytorch_pretrained_bert) (1.25.11)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.15->boto3->pytorch_pretrained_bert) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.15->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.5.30)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement bertModel (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for bertModel\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7n8akrq3zLW3","executionInfo":{"status":"ok","timestamp":1628262402053,"user_tz":240,"elapsed":396,"user":{"displayName":"labesh baral","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgKfkRAcC4q8O4AOIGKbBTAqtxCx15FOR_A-b0=s64","userId":"09665838403334475686"}}},"source":["from tweepy.streaming import StreamListener\n","from tweepy import API, Cursor, Stream, OAuthHandler\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib\n","from textblob import TextBlob\n","import re\n","import datetime\n","from nltk.tokenize import WordPunctTokenizer\n","from bs4 import BeautifulSoup\n","tok = WordPunctTokenizer()\n","pat1 = r'@[A-Za-z0-9]+'\n","pat2 = r'https?://[A-Za-z0-9./]+'\n","combined_pat = r'|'.join((pat1, pat2))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"rWMoqEt8sTLH","executionInfo":{"status":"error","timestamp":1628262402053,"user_tz":240,"elapsed":4,"user":{"displayName":"labesh baral","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgKfkRAcC4q8O4AOIGKbBTAqtxCx15FOR_A-b0=s64","userId":"09665838403334475686"}},"outputId":"f2d5c6c1-445d-40af-f0c5-1d51aebd3cc9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import torch\n","import torch.nn.functional as F\n","from pytorch_pretrained_bert import BertTokenizer\n","from bertModel import BertClassification"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-054aaad160a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertModel'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"MKMHUaRKC0uH"},"source":["# Pre-trained FinBERT"]},{"cell_type":"code","metadata":{"id":"C3bSxI-2BBV9"},"source":["labels = {0:'neutral', 1:'positive',2:'negative'}\n","num_labels= len(labels)\n","vocab = \"finance-uncased\"\n","vocab_path = '/content/drive/MyDrive/analyst_tone/vocab'\n","pretrained_weights_path = \"/content/drive/MyDrive/analyst_tone/pretrained_weights\" # this is pre-trained FinBERT weights\n","fine_tuned_weight_path = \"/content/drive/MyDrive/analyst_tone/fine_tuned.pth\"      # this is fine-tuned FinBERT weights\n","max_seq_length=512\n","device='cuda:0'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFhGwV-_BEcq"},"source":["model = BertClassification(weight_path= pretrained_weights_path, num_labels=num_labels, vocab=vocab)\n","model.load_state_dict(torch.load(fine_tuned_weight_path, 'cuda:0'))\n","model.to(device)\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9lngwHCcB87e"},"source":["sentences = [\"There is a shortage of capital, and we need extra financing\", \n","             \"Growth is strong and we have plenty of liquidity.\", \n","             \"There are doubts about our finances.\", \n","             \"Facebook is going down bad.\"]\n","tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"153LtB5-CBgS"},"source":["def sa(sent):\n","  tokenized_sent = tokenizer.tokenize(sent)\n","  if len(tokenized_sent) > max_seq_length:\n","      tokenized_sent = tokenized_sent[:max_seq_length]\n","    \n","  ids_review  = tokenizer.convert_tokens_to_ids(tokenized_sent)\n","  mask_input = [1]*len(ids_review)        \n","  padding = [0] * (max_seq_length - len(ids_review))\n","  ids_review += padding\n","  mask_input += padding\n","  input_type = [0]*max_seq_length\n","    \n","  input_ids = torch.tensor(ids_review).to(device).reshape(-1, max_seq_length)\n","  attention_mask =  torch.tensor(mask_input).to(device).reshape(-1, max_seq_length)\n","  token_type_ids = torch.tensor(input_type).to(device).reshape(-1, max_seq_length)\n","    \n","  with torch.set_grad_enabled(False):\n","      outputs = model(input_ids, token_type_ids, attention_mask)\n","      outputs = F.softmax(outputs,dim=1)\n","      return labels[torch.argmax(outputs).item()]\n","      #print(sent, '\\nFinBERT predicted sentiment: ', outputs, '\\n')\n","      #neutral, pos, neg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"swp-bJL_0LEl"},"source":["# Classes"]},{"cell_type":"markdown","metadata":{"id":"0K4H4uPsOLLF"},"source":["## TwitterAuthenticator Class"]},{"cell_type":"code","metadata":{"id":"p_N8TvxmOOf3"},"source":["class TwitterAuthenticator():\n","  def authenticate_twitter_app(self):\n","    auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n","    auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n","    return auth"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3lI5beofOXHb"},"source":["## TwitterListener Class"]},{"cell_type":"code","metadata":{"id":"Ji3v-VSn0MOk"},"source":["class TwitterListener(StreamListener): # Inherits from StreamListener Class\n","  '''\n","  Simple listener class that just prints received tweets to standard output.\n","  '''\n","  \n","  def __init__(self, fetched_tweets_filename):\n","    self.fetched_tweets_filename = fetched_tweets_filename \n","\n","  def on_data(self, data):\n","    try:\n","      print(data)\n","      with open(self.fetched_tweets_filename, 'a') as tf:\n","        tf.write(data)\n","      return True\n","    except BaseException as e:\n","      print(\"Error on_data %s\" % str(e))\n","    return true\n","\n","  def on_error(self, status):\n","    if status == 420:\n","      # Case rate limit occurs\n","      return False;\n","    print(status)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7cAdyXifcOR"},"source":["## TweetAnalyzer Class"]},{"cell_type":"code","metadata":{"id":"-yWNXowAffaE"},"source":["class TweetAnalyzer():\n","  def tweets_to_df(self, tweets):\n","    df = pd.DataFrame(data=[tweet.full_text for tweet in tweets], columns=[\"tweets\"])\n","    df['id'] = np.array([tweet.id for tweet in tweets])\n","    df['len'] = np.array([len(tweet.full_text) for tweet in tweets])\n","    df['date'] = np.array([tweet.created_at for tweet in tweets])\n","    df['source'] = np.array([tweet.source for tweet in tweets])\n","    df['likes'] = np.array([tweet.favorite_count for tweet in tweets])\n","    df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n","    return df\n","\n","  def relating(self, t):\n","    df2 = t['id']\n","    df2['tweets'] = np.array(t['text'])\n","    df2['date'] = np.array(t['created_at'])\n","    df2['source'] = np.array(t['source'])\n","    df2['likes'] = np.array(t['favorite_count'])\n","    df2['retweets'] = np.array(t['retweet_count'])\n","    return df2\n","\n","  def clean_tweet(self, text):\n","    text = re.sub('\\\\n', '', text)\n","    text = re.sub('https?:\\/\\/\\S+', u'', text)\n","    text = re.sub('\\xa0', u'', text)\n","    soup = BeautifulSoup(text, 'lxml')\n","    souped = soup.get_text()\n","    stripped = re.sub(combined_pat, '', souped)\n","    try:\n","        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n","    except:\n","        clean = stripped\n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n","    lower_case = letters_only.lower()\n","    # During the letters_only process two lines above, it has created unnecessay white spaces,\n","    # I will tokenize and join together to remove unneccessary white spaces\n","    words = tok.tokenize(lower_case)\n","    return (\" \".join(words)).strip()\n","\n","  def analyze_sentiment(self, tweet):\n","    #analysis = TextBlob(self.clean_tweet(tweet))\n","    #return analysis.sentiment.polarity\n","    twt = self.clean_tweet(tweet)\n","    return sa(twt)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z_42f65GOF66"},"source":["## TwitterClient Class"]},{"cell_type":"code","metadata":{"id":"aHCFcmpcOIgn"},"source":["class TwitterClient():\n","  def __init__(self, twitter_user=None): # When you do the None, it means default. If no user is specified, it defaults to you.\n","    self.auth = TwitterAuthenticator().authenticate_twitter_app()\n","    self.twitter_client = API(self.auth)\n","    \n","    self.twitter_user = twitter_user\n","  \n","  def get_user_timeline_tweets(self, num_tweets):\n","    tweets = []\n","    search_term = '$INTC AND (buy OR sell) AND Intel -filter:retweets'\n","    for tweet in Cursor(api.search, q=search_term, lang = 'en', since = '2021-07-28', tweet_mode='extended').items(num_tweets): # The API provides a timeline for every user, which gets the tweets from a user.\n","      tweets.append(tweet)\n","    return tweets\n","\n","  def get_home_timeline_tweets(self, num_tweets):\n","        home_timeline_tweets = []\n","        for tweet in Cursor(self.twitter_client.home_timeline, id=self.twitter_user).items(num_tweets):\n","            home_timeline_tweets.append(tweet)\n","        return home_timeline_tweets\n","\n","  def get_twitter_client_api(self):\n","    return self.twitter_client"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p9Sqmv4DOPBD"},"source":["## TwitterStreamer Class"]},{"cell_type":"code","metadata":{"id":"obvIDKbjOVfL"},"source":["class TwitterStreamer():\n","  '''\n","  Class for streaming and processing live tweets\n","  '''\n","\n","  def __init__(self):\n","    self.twitter_authenticator = TwitterAuthenticator()\n","\n","  def stream_tweets(self, fetched_tweets_filename, hash_tag_list):\n","    # Handles Twitter Auth and connects to the Twitter Streaming API.\n","    listener = TwitterListener(fetched_tweets_filename)\n","    auth = self.twitter_authenticator.authenticate_twitter_app()\n","    \n","    stream = Stream(auth, listener)\n","    # stream.filter(track=['donald trump', 'hillary clinton', 'bernie sanders', 'barack obama'])\n","    stream.filter(track=hash_tag_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4uwViHSrOfj4"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"bOeCR9RbOgko"},"source":["if __name__ == \"__main__\":\n","  #twitter_client = TwitterClient('TheRealPatD123') # Put person here.\n","  #print(twitter_client.get_user_timeline_tweets(2)) # Can get number of pages of tweets for people, not just the number of tweets\n","\n","  # Getting Tweets from a specific person and doing SA on it.\n","  twitter_client = TwitterClient()\n","  api = twitter_client.get_twitter_client_api()\n","\n","  #tweets = api.user_timeline(screen_name='HillaryClinton', count=2)\n","  tweets = twitter_client.get_user_timeline_tweets(200)\n","  tweet_analyzer = TweetAnalyzer()\n","  df = tweet_analyzer.tweets_to_df(tweets)\n","  \n","  # print(tweets[0].favorite_count)\n","\n","  #TIME SERIES\n","  #time_likes = pd.Series(df['likes'].values, index=df['date'])\n","  #time_likes.plot(figsize=(16, 4), label='likes', legend=True)\n","  #time_retweets = pd.Series(df['retweets'].values, index=df['date'])\n","  #time_retweets.plot(figsize=(16, 4), label='retweets', legend=True)\n","  #plt.show()\n","  \n","  # Sentiment Analysis\n","  df['sentiment'] = [tweet_analyzer.analyze_sentiment(tweet.full_text) for tweet in tweets]\n","  # df['clean_tweets'] = [tweet_analyzer.clean_tweet(df.iloc[i]['tweets']) for i in range(0, df.shape[0])]\n","  print(df)\n","\n","  # Filtering for certain tweets\n","  #hash_tag_list = ['donald trump']\n","  #fetched_tweets_filename = \"tweets.json\"\n","  #twitter_streamer = TwitterStreamer()\n","  #twitter_streamer.stream_tweets(fetched_tweets_filename, hash_tag_list)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rmrWzWk9nWR"},"source":["from statistics import median\n","\n","df['date'] = matplotlib.dates.date2num(df['date'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OkHjojsEu-RB"},"source":["for i in range(0, df.shape[0]):\n","  print(df.iloc[i]['sentiment'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iLOJh0XwW78"},"source":["for i in range(df.shape[0] - 1, -1, -1):\n","  #df.iloc[i]['date'] = round(df.iloc[i]['date'], 3)\n","  if df.iloc[i]['sentiment'] == 0:\n","    df = df.drop(i, 0)\n","\n","df = df.sort_values(by =['date'])\n","\n","l = []\n","for i in range(0, df.shape[0] - 1):\n","  l.append(df.iloc[i]['sentiment'])\n","\n","  if df.iloc[i]['date'] != df.iloc[i + 1]['date']:\n","    df.iloc[i]['sentiment'] = median(l)\n","    l.clear()\n","  else:\n","    df = df.drop(i, 0)\n","\n","\n","\n","plt.plot_date(df['date'], df['sentiment'])\n","plt.gcf().autofmt_xdate()\n","# plt.xticks(ticks=np.arange(datetime(2021, 7,27), datetime(2021, 8,3), timedelta(days = 2)))\n","plt.minorticks_on();\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S7_LNKZpwQt-"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYhU6_cJlM8k"},"source":["for i in range (0, df.shape[0]):\n","  print(i)\n","  print(df.iloc[i]['tweets'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jCj-O1p-IRz"},"source":["#g = pd.read_json('tweets.json', lines=True)\n","#h = relating(g)\n","#print(h)\n","#h['sentiment'] = [tweet_analyzer.analyze_sentiment(g.iloc[i]['text']) for i in range(0, g.shape[0])]\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MP0gEEEl18n"},"source":["print(dir(tweets[0])) # Shows the number of options you can access from one tweet(user, text, place, retweet count, etc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mbGUfSCkbCP"},"source":[""],"execution_count":null,"outputs":[]}]}